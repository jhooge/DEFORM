{
    "collab_server" : "",
    "contents" : " ---\ntitle: \"Biomedical Topic Modeling on Cancer Immunotherapy\"\nauthor: \"Jens Hooge\"\ndate: \"16. Februar 2016\"\noutput: html_document\n---\n\n# Introduction\nTopic models are probabilistic latent variable models of documents that exploit the correlations among the words and latent semantic themes” (Blei and Lafferty, 2007). The name \"topics\" signifies the hidden, to be estimated, variable relations (=distributions) that link words in a vocabulary and their occurrence in documents. A document is seen as a mixture of topics. This intuitive explanation of how documents can be generated is modeled as a stochastic process which is then \"reversed\"\" (Blei and Lafferty, 2009) by machine learning techniques that return estimates of the latent variables. With these estimates it is possible to perform information retrieval or text mining tasks on a document corpus.\n\n# Loading required libraries\nIn this study we will utilize R tm R package for querying and textmining of the NIPS Papers 2015\n```{r, warning=FALSE, message=FALSE}\nlibrary(tm) ## texmining\nlibrary(lda) ## the actual LDA model\nlibrary(LDAvis) # visualization library for LDA\nlibrary(RISmed)\n\nlibrary(parallel) # multi-core paralellization\n\nlibrary(data.table) # fread\n##library(Rmpfr) # harmonic mean maximization\nlibrary(ggplot2) # pretty plotting lib\nlibrary(reshape2) # reformatting lib for ggplot2\n\nlibrary(tsne) # low dimensional embedding\nlibrary(caret) # ml model wrapper lib, but only used for data transformation here\n\nload(\"immunoTopicModels.rda\")\nload(\"vocab.rda\")\nload(\"termTable.rda\")\nload(\"documents.rda\")\nload(\"LMats.rda\")\n```\n\n# Helper Functions\n```{r}\n#' Copy arguments into env and re-bind any function's lexical scope to bindTargetEnv .\n#' \n#' See http://winvector.github.io/Parallel/PExample.html for example use.\n#' \n#' \n#' Used to send data along with a function in situations such as parallel execution \n#' (when the global environment would not be available).  Typically called within \n#' a function that constructs the worker function to pass to the parallel processes\n#' (so we have a nice lexical closure to work with).\n#' \n#' @param bindTargetEnv environment to bind to\n#' @param objNames additional names to lookup in parent environment and bind\n#' @param names of functions to NOT rebind the lexical environments of\nbindToEnv <- function(bindTargetEnv=parent.frame(), objNames, doNotRebind=c()) {\n  # Bind the values into environment\n  # and switch any functions to this environment!\n  for(var in objNames) {\n    val <- get(var, envir=parent.frame())\n    if(is.function(val) && (!(var %in% doNotRebind))) {\n      # replace function's lexical environment with our target (DANGEROUS)\n      environment(val) <- bindTargetEnv\n    }\n    # assign object to target environment, only after any possible alteration\n    assign(var, val, envir=bindTargetEnv)\n  }\n}\n\nstartCluster <- function(cores=detectCores()) {\n  cluster <- makeCluster(cores)\n  return(cluster)\n}\n\nshutDownCluster <- function(cluster) {\n  if(!is.null(cluster)) {\n    stopCluster(cluster)\n    cluster <- c()\n  }\n}\n```\n\n# Read the NIPS 2015 Corpus\n\nFirst we will read all the nips papers in a data frame. We will use the abstracts for the training of our LDA.\n```{r}\npapers = fread(\"~/Datasets/kaggle/NIPS2015_papers/Papers.csv\")\ndocs = papers$Abstract\n```\n\n# Preprocessing\nTo train the LDA in the later steps, we need the word frequencies in each of those abstracts. For representative word frequencies we removed a number of problematic characters, removed punctuation, control characters, whitespaces, stopwords which belonged to the SMART stopword collection, all words with less than 4 characters and words which occurred less than 4 times in the documents. Lastly we transformed each word to lowercase.\n\n```{r, eval=FALSE}\nstop_words <- stopwords(\"SMART\")\ndocs <- gsub(\"[[:punct:]]\", \" \", docs)  # replace punctuation with space\ndocs <- gsub(\"[[:cntrl:]]\", \" \", docs)  # replace control characters with space\ndocs <- gsub(\"^[[:space:]]+\", \"\", docs) # remove whitespace at beginning of documents\ndocs <- gsub(\"[[:space:]]+$\", \"\", docs) # remove whitespace at end of documents\ndocs <- tolower(docs)  # force to lowercase\n\n# tokenize on space and output as a list:\ndoc.list <- strsplit(docs, \"[[:space:]]+\")\n\n# Remove all words with less than 4 characters\ndoc.list <- lapply(doc.list, function(x) x[sapply(x, nchar)>3])\n\n# compute the table of terms:\nterm.table <- table(unlist(doc.list))\nterm.table <- sort(term.table, decreasing = TRUE)\n\n# remove terms that are stop words or occur fewer than 5 times:\ndel <- names(term.table) %in% stop_words | term.table < 5\nterm.table <- term.table[!del]\nvocab <- names(term.table)\n\nsave(vocab, file=\"vocab.rda\")\nsave(term.table, file=\"termTable.rda\")\n```\n\nNext we reformated the documents into the format required by the lda package.\n```{r, eval=FALSE}\nget.terms <- function(x) {\n  index <- match(x, vocab)\n  index <- index[!is.na(index)]\n  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\n}\ndocuments <- lapply(doc.list, get.terms)\n\nsave(documents, file=\"documents.rda\")\n```\n\nBefore we start training our LDA, we first will calculate some statistics related to the data set:\n```{r}\nD <- length(documents)  # number of documents\nW <- length(vocab)  # number of terms in the vocab\ndoc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document\nN <- sum(doc.length)  # total number of tokens in the data\nterm.frequency <- as.integer(term.table)  # frequencies of terms in the corpus\n```\n\n```{r}\ndf <- data.frame(\"Number of Documents\"=D, \n                 \"Number of Terms in Vocabulary\"=W,\n                 \"Total Number of Tokens in Corpus\"=N)\n\nknitr::kable(df, digits = 0, caption = \"Document Statistics\")\n```\n\n# LDA Parameters\nNow we can define the parameters for our LDA Model. We will train LDA models assuming between 2 and 10 topics in out document corpus.\nTODO: Explain the different Parameters (see notes and http://videolectures.net/mlss09uk_blei_tm/)\nTODO: Tune the hyperparameters too, using expand.grid\nTODO: Fix harmonic mean normalization method. Harmonic mean always decreases with increasing number of topics. This cant be right.\n\nFor the symmetric dirichlet distribution, a high alpha-value means that each document is likely to contain a mixture of most of the topics, and not any single topic specifically. A low alpha value puts less such constraints on documents and means that it is more likely that a document may contain mixture of just a few, or even only one, of the topics. Likewise, a high beta-value means that each topic is likely to contain a mixture of most of the words, and not any word specifically, while a low value means that a topic may contain a mixture of just a few of the words.\n\nIf, on the other hand, the distribution is asymmetric, a high alpha-value means that a specific topic distribution (depending on the base measure) is more likely for each document. Similarly, high beta-values means each topic is more likely to contain a specific word mix defined by the base measure.\n\nIn practice, a high alpha-value will lead to documents being more similar in terms of what topics they contain. A high beta-value will similarly lead to topics being more similar in terms of what words they contain. (source: http://stats.stackexchange.com/questions/37405/natural-interpretation-for-lda-hyperparameters)\n\n```{r}\nG <- 100 ## number of iterations\n## alpha and eta are hyperparameters contring the sparsity of the document/topic matrix (theta)\n## and the word/topic (lambda) sparsity\n# alpha   ## Papers are very specific such that a low alpha is more probable.\n# eta     ## The scalar value of the Dirichlet hyperparamater for topic multinomials.\nk = seq(2, 20, by=1) ## The number of topics a document contains.\n\n## We will define parameter sets for 2400 LDA models, \n## which can be trained in about 1 hour using 8 cpu threads‚\nparams <- expand.grid(G=G,\n                      k=k,\n                      alpha=seq(0.01, 0.5, length.out=10),\n                      eta=seq(0.01, 0.5, length.out=10))\nparams <- setNames(split(params, seq(nrow(params))), rownames(params))\n```\n\n# Model Tuning\nWith the parameters defined above we can now go on and train our set of models. For parallel computing we will define our worker function, which will bind all variables needed during training to the global environment, such that they are available for each core.\nTODO: Proper explaination on http://www.win-vector.com/blog/2016/01/parallel-computing-in-r/\n\n# Multi CPU Computation\n```{r, eval=FALSE}\nset.seed(357)\n\n# worker <- function() {\n#   bindToEnv(objNames=c(\"documents\", \"vocab\", \"G\", \"alpha\", \"eta\"))\n#   function(k) {\n#     lda::lda.collapsed.gibbs.sampler(documents = documents, K = k, vocab = vocab, \n#                                      num.iterations = G, alpha = alpha, \n#                                      eta = eta, initial = NULL, burnin = 0,\n#                                      compute.log.likelihood = TRUE)\n#   }\n# }\n\nworker <- function() {\n  bindToEnv(objNames=c(\"documents\", \"vocab\"))\n  function(params) {\n    k <- params$k\n    G <- params$G\n    alpha <- params$alpha\n    eta <- params$eta\n    lda::lda.collapsed.gibbs.sampler(documents = documents, K = k, vocab = vocab, \n                                     num.iterations = G, alpha = alpha, \n                                     eta = eta, initial = NULL, burnin = 0,\n                                     compute.log.likelihood = TRUE)\n  }\n}\n\nt1 <- Sys.time()\ncluster <- startCluster()\n# models <- parLapply(cluster, ntopics, worker())\nmodels <- parLapply(cluster, params, worker())\nshutDownCluster(cluster)\nt2 <- Sys.time()\nt2 - t1\n\nsave(models, file=\"ldaModels_NIPS2015.rda\")\n```\n\n# Single CPU Computation\n```{r, eval=FALSE}\nt1 <- Sys.time()\nmodels = lapply(ntopics, lda::lda.collapsed.gibbs.sampler(documents = documents, K = k, vocab = vocab, \n                                                          num.iterations = params$G, alpha = params$alpha,\n                                                          eta = params$eta, initial = NULL, burnin = 0,\n                                                          compute.log.likelihood = TRUE))\nt2 <- Sys.time()\nt2 - t1\n```\n\n# Convergence\nTo first let's have a look whether our models have converged after `r G` iterations. The following figure shows\nthe log-likelihood in each iteration of each model assuming a number of topics. The mean log likelihood over all models is depicted in red\n\n```{r, echo=FALSE}\nlogLiks = lapply(models, function(L)  L$log.likelihoods[1,])\nlogLiks.df <- as.data.frame(logLiks)\nmeanLogLiks <- data.frame(Iteration=as.numeric(1:nrow(logLiks.df)), \n                          logLikelihood=rowMeans(logLiks.df))\nlogLiks.df$Iteration <- rownames(logLiks.df)\ncolnames(logLiks.df) <- 1:ncol(logLiks.df)\n\nmolten_logLiks <- melt(logLiks.df)\ncolnames(molten_logLiks) <- c(\"Iteration\", \"Model\", \"logLikelihood\")\nmolten_logLiks$Iteration <- as.numeric(molten_logLiks$Iteration)\n\nggplot(data=molten_logLiks) + \n  geom_line(aes(x=Iteration, y=logLikelihood), alpha=0.2) +\n  geom_line(data = meanLogLiks, aes(x=Iteration, y=logLikelihood), \n            color=\"black\", linetype=\"dashed\") +\n  theme_bw()\n```\n\n# Model Selection\nWe will select our model based on harmonic mean maximization.\nTODO: Check http://epub.wu.ac.at/3558/1/main.pdf for proper explanation and comparison to other performance values.\nTODO: Fix harmonic mean calculation.\n```{r}\nharmonicMean = function(logLikelihoods, precision=2000L) {\n  llMed = median(logLikelihoods)\n  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,\n                                       prec = precision) + llMed))))\n}\n\nparams <- expand.grid(G=G,\n                      k=k,\n                      alpha=seq(0.01, 100, length.out=10),\n                      eta=seq(0.01, 100, length.out=10))\n\n## TODO: Check whether the matrix is filled in the right order\n## returns a grid of harmonic mean log likelihoods\n## for a fixed number of topics k and a fixed number of iterations\n## gradient lines are plotted against the LDA hyperparameters alpha and eta\nparamMatrix <- function(k, params, models) {\n  topic.params <- params[which(params$k==k),]\n  indexes <- as.numeric(rownames(topic.params))\n  alpha <- unique(topic.params$alpha)\n  eta <- unique(topic.params$eta)\n  selectedModels <- models[indexes]\n  logLiks = lapply(selectedModels, function(m)  m$log.likelihoods[1, ])\n  harmMeanLogLiks <- sapply(logLiks, function(h) harmonicMean(h))\n  m <- matrix(harmMeanLogLiks, nrow=length(alpha), ncol=length(eta))\n  rownames(m) <- alpha\n  colnames(m) <- eta\n  return(m)\n}\n\n## Test whether matrix has been filled correctly and axis labels are correct\n# topic.params <- params[which(params$k==3),]\n# print(topic.params)\n# indexes <- as.numeric(rownames(topic.params))\n# alpha <- unique(topic.params$alpha)\n# eta <- unique(topic.params$eta)\n# selectedModels <- models[indexes]\n# logLiks = lapply(selectedModels, function(m)  m$log.likelihoods[1, ])\n# harmMeanLogLiks <- sapply(logLiks, function(h) harmonicMean(h))\n#\n# m <- melt(m_3)\n# colnames(m) <- c(\"alpha\", \"eta\", \"HarmonicMeanlogLikelihood\" )\n# \n# ggplot(data=m, aes(x=alpha, y=eta, z=HarmonicMeanlogLikelihood)) +\n#   stat_contour(bins=20, aes(colour = ..level..)) + \n#   theme_bw()\n#\n# harmonicMean(models[[137]]$log.likelihoods[1, ]) ## alpha=55.56, eta=11.12 --> -218668.9\n# harmonicMean(models[[47]]$log.likelihoods[1, ])  ## alpha=55.56, eta=0.01 --> should be smaller than -218668.9\n# harmonicMean(models[[173]]$log.likelihoods[1, ])  ## alpha=100.00, eta=11.12 --> should be close to -218668.9\n\n## contour matrices for each k over alpha and eta hyper-parameters\ncontourMatrices <- lapply(unique(params$k), paramMatrix, params, models)\n## vector of argmax(likelihood) indices over all alphas of each contour matrix\nopt_alphas <- sapply(contourMatrices, function(m) which(rowSums(m)==max(rowSums(m))))\n## vector of argmax(likelihood) indices over all etas of each contour matrix\nopt_etas   <- sapply(contourMatrices, function(m) which(colSums(m)==max(colSums(m))))\nlogLiks    <- sapply(contourMatrices, function(m) max(apply(m, 1, max)))\n## Combine everything in a data frame including the optimal parameters over all LDAs with \n## a fixed k and number of iterations G\nopt_params <- data.frame(k=unique(params$k), \n                         alpha=names(opt_alphas), \n                         eta=names(opt_etas),\n                         logLikelihood=logLiks)\n\nknitr::kable(opt_params, caption = \"Optimal LDA hyper-parameter settings over all k and a fixed number of iterations G\")\n\nggplot(data=opt_params, aes(x=k, y=logLikelihood)) +\n  geom_line() +\n  theme_bw()\n```\n\nThe further analysis will only consider the model with the maximum log-likelihood depicted in the table above.\n\n```{r, eval=FALSE}\n## This will select a model with 2 topics. This doesn't make sense, given some exploration beforehand\n## Especially with regard to the t-SNE dimensionality reduction which identifies about 8-9 clusters in the corpus.\nparams <- as.data.frame(apply(params, 2, as.character))\n\ni <- which(opt_params$logLikelihood==max(opt_params$logLikelihood))\nopt_k <- as.character(opt_params[i, ]$k)\nopt_alpha <- as.character(opt_params[i, ]$alpha)\nopt_eta <- as.character(opt_params[i, ]$eta)\nopt_model_index <- as.numeric(rownames(subset(params, (k==opt_k & \n                                                       alpha==opt_alpha & \n                                                       eta==opt_eta))))\nopt_k <- as.numeric(opt_k)\nopt_alpha <- as.numeric(opt_alpha)\nopt_eta <- as.numeric(opt_eta)\nmodel <- models[[opt_model_index]]\n```\n\n\n\n```{r, echo=FALSE, eval=FALSE}\nm <- melt(contourMatrices[[i]])\ncolnames(m) <- c(\"alpha\", \"eta\", \"HarmonicMeanlogLikelihood\" )\n\nggplot(data=m, aes(x=alpha, y=eta, z=HarmonicMeanlogLikelihood)) +\n  geom_vline(xintercept = opt_alpha, colour=\"red\", linetype = \"longdash\", alpha=0.5) +\n  geom_hline(yintercept = opt_eta, colour=\"red\", linetype = \"longdash\", alpha=0.5) +\n  stat_contour(bins=20, aes(colour = ..level..)) +\n  theme_bw()\n```\n\nThe optimal model is described by the following parameters: alpha=`r opt_alpha` and eta=`r opt_eta`.\n\n# Get the top 5 words defining the first 5 topics\n```{r, eval=FALSE}\nN <- 5 \ntop.words <- top.topic.words(model$topics, 5, by.score=TRUE)\ntop.words.df <- as.data.frame(top.words)\ncolnames(top.words.df) <- 1:opt_k\n\nknitr::kable(top.words.df[ ,1:opt_k], caption = \"Top 5 terms per topic\")\n```\n\n# Get the top 5 documents assigned to the first 5 topics\n```{r, eval=FALSE, echo=FALSE} \ntop.documents <- top.topic.documents(model$document_sums, \n                                     num.documents = 20, \n                                     alpha = opt_alpha)\ntop.documents.df <- as.data.frame(top.documents)\ncolnames(top.documents.df) <- 1:opt_k\n\ntop.documents.df.part <- head(top.documents.df, 10)\ntopic_titles <- data.frame(lapply(1:opt_k, function(k) papers[as.numeric(top.documents.df.part[ ,k]),]$Title))\ncolnames(topic_titles) <- 1:opt_k\n\nknitr::kable(topic_titles, caption = \"Top 10 titles per topic\")\n```\n\n# Get maximum proportion topic for each document\nFirst we will compute to which proportion a document belongs to a topic. As zero values and NAs will be a problem in the succeeding steps we will add a small number to each element in the topic proportion matrix. The topic with the maximum proportion value will then be assigned to the document. Proportion is a measure indicating the number of times words in each document were assigned to each topic. \n```{r, eval=FALSE}\ntopic.proportions <- t(model$document_sums) / colSums(model$document_sums)\ntopic.proportions <- topic.proportions + 0.000000001 ## Laplace smoothing\n\ngetTopic <- function(topic.vec) {\n  argmax <- which(topic.vec==max(topic.vec))\n  if(length(argmax)>1){\n    argmax <- argmax[1]\n  }\n  return(argmax)\n}\n\ngetTopicProportion <- function(topic.vec) {\n  return(topic.vec[getTopic(topic.vec)])\n}\n\n# assign each document the topic with maximum probability\ndoc.topic <- unlist(apply(topic.proportions, 1, getTopic))\ndoc.maxTopicProportion <- unlist(apply(topic.proportions, 1, getTopicProportion))\ndoc.topic.words <- apply(top.words[, doc.topic], 2, paste, collapse=\".\")\n```\n\nEach document can be seen as a mixture of topics as exemplified in the figure below.\n```{r, echo=FALSE, eval=FALSE}\nN <- 4\ntp <- topic.proportions[sample(1:dim(topic.proportions)[1], N),]\ncolnames(tp) <- apply(top.words, 2, paste, collapse=\" \")\ntp.df <- melt(cbind(data.frame(tp),\n                    document=factor(1:N)),\n              variable.name=\"topic\",\n              id.vars = \"document\")  \n\n\nggplot(data=tp.df, aes(x=topic, y=value, fill=document)) +\n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle=90, hjust=1)) +\n  coord_flip() +\n  facet_wrap(~ document, ncol=2) +\n  theme_bw()\n```\n\n# Compute similarity between documents\nA document is defined as a mixture of topics, each with a certain probability. In other words a document is to some proportion, part of each topic. Given two proportion vectors, a similarity can be computed between two documents. A similarity measure between two probability distributions, can be computed using the Jensen-Shannon divergence (JSD) (TODO: citation), which can be derived from the Kullback-Leibler divergence. (TODO: LaTeX formula of JSD) The JSD is defined as follows\n\n```{r, eval=FALSE}\n## Compute Jensen-Shannon Divergence between documents\n## p,q probability distribution vectors R^n /in [0,1]\nJSD <- function(p, q) {\n  m <- 0.5 * (p + q)\n  divergence <- 0.5 * (sum(p * log(p / m)) + sum(q * log(q / m)))\n  return(divergence)\n}\n```\n\nWith this we can compute the pairwise similarity between each document.\n```{r, eval=FALSE}\nn <- dim(topic.proportions)[1]\nX <- matrix(rep(0, n*n), nrow=n, ncol=n)\nindexes <- t(combn(1:nrow(topic.proportions), m=2))\nfor (r in 1:nrow(indexes)) {\n  i <- indexes[r, ][1]\n  j <- indexes[r, ][2]\n  p <- topic.proportions[i, ]\n  q <- topic.proportions[j, ]\n  X[i, j] <- JSD(p,q) \n}\nX <- X+t(X)\n```\n\n```{r}\nX_dist <- sqrt(X) # compute Jensen-Shannon Distance\n```\n\n## Clustering and Dimension Reduction of Jensen-Shannon Distance matrix\nTo visualize the results we have to reduce the dimensionality of our document similarity matrix. To do this we need a distance matrix. Taking the square root of the JSD matrix results in a metric called Jensen-Shannon distance, which can be used in hirarchical clustering as well as, dimensionality reduction algorithms.\n```{r, eval=FALSE, echo=FALSE}\nlibrary(apcluster)\n## run affinity propagation\napres <- apcluster(X_dist, details=TRUE)\nshow(apres)\n\n## plot information about clustering run\nplot(apres)\n\n## plot clustering result\nplot(apres, X_dist)\n\n## employ agglomerative clustering to join clusters\naggres <- aggExCluster(sim, apres)\n\n## show information\nshow(aggres)\nshow(cutree(aggres, 2))\n\n## plot dendrogram\nplot(aggres)\n\n## plot clustering result for k=2 clusters\nplot(aggres, X_dist, k=2)\n\n## plot heatmap\nheatmap(apres, sim)\n```\n\nFor exploratory purposes we will embedd the distance matrix onto a 2-dimensional plane using different projection methods, Multidimensional Scaling (TODO: citation), Principal Component Analysis (TODO: cite) and t-SNE (TODO: cite).\n\nTODO: init_dims and perplexity has to be tuned for t-SNE. It is yet unclear how to do this properly\n```{r, eval=FALSE}\nworker <- function() {\n  bindToEnv(objNames=c(\"topic.proportions\", \"opt_k\"))\n  function(perplexity) {\n    tsne::tsne(topic.proportions, k=2, initial_dims=opt_k+1, perplexity=perplexity)\n  }\n}\n\nperplexities <- seq(5, 50, by=5)\nt1 <- Sys.time()\ncluster <- startCluster()\nX_tSNE_projections <- parLapply(cluster, perplexities, worker())\nshutDownCluster(cluster)\nt2 <- Sys.time()\nt2 - t1\n\nX_tSNE_projected <- X_tSNE_projections[[1]]\n\n\nX_MDS_projected <- cmdscale(X_dist, k = 2) ## Multi dimensional scaling\n# X_tSNE_projected <- tsne(topic.proportions, k = 2, initial_dims = opt_k+1, perplexity = 40) ## t-SNE projection\npreProc = preProcess(topic.proportions, method=c(\"center\", \"scale\", \"pca\"))\nX_PCA_projected = predict(preProc, topic.proportions)[,1:2] # PCA projection\n\nprojections <- data.frame(Topic=as.factor(doc.topic), \n                          TopWords=as.factor(doc.topic.words),\n                          Proportion=doc.maxTopicProportion,\n                          Title=papers$Title,\n                          EventType=papers$EventType,\n                          Abstract=papers$Abstract,\n                          x_pca=X_PCA_projected[, 1], \n                          y_pca=X_PCA_projected[, 2],\n                          x_mds=X_MDS_projected[, 1], \n                          y_mds=X_MDS_projected[, 2],\n                          x_tsne=X_tSNE_projected[, 1], \n                          y_tsne=X_tSNE_projected[, 2])\n```\n\nNow let's have a look at the results using the interactive plotting library rbokeh, with which it is possible to select certain clusters in each projection, a method called linked brushing.\n\n```{r, eval=FALSE}\ntools <- c(\"pan\", \n           \"wheel_zoom\", \"box_zoom\", \n           \"box_select\", \"lasso_select\", \n           \"reset\", \"save\")                            \n## PCA Plot\npca_fig <- figure(tools=tools) %>%\n  ly_points(x_pca, y_pca, data = projections,\n            color = Topic, size = Proportion*10,\n            hover = list(TopWords, Proportion, Title, \n                         EventType))\n## MDS Plot\nmds_fig <- figure(tools=tools) %>%\n  ly_points(x_mds, y_mds, data = projections,\n            color = Topic, size = Proportion*10,\n            hover = list(TopWords, Proportion, Title, \n                         EventType))\n## t-SNE Plot\ntsne_fig <- figure(tools=tools) %>%\n  ly_points(x_tsne, y_tsne, data = projections,\n            color = Topic, size = Proportion*10,\n            hover = list(TopWords, Proportion, Title, \n                         EventType))\n\nprojList <- list(pca_fig, mds_fig, tsne_fig)\np = grid_plot(projList, ncol=2, link_data=TRUE)\np\n```\n\n# LDAVis Visualization\n```{r, eval=FALSE}\ntheta <- t(apply(model$document_sums + opt_alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(model$topics) + opt_eta, 2, function(x) x/sum(x)))\n\nmodelParams <- list(phi = phi,\n                      theta = theta,\n                      doc.length = doc.length,\n                      vocab = vocab,\n                      term.frequency = term.frequency)\n\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = modelParams$phi, \n                   theta = modelParams$theta, \n                   doc.length = modelParams$doc.length, \n                   vocab = modelParams$vocab, \n                   term.frequency = modelParams$term.frequency)\n\nserVis(json, out.dir = \"vis\", open.browser = FALSE)\n```\n\n```{r, echo=FALSE, eval=FALSE}\ntmp <- URLencode(paste(readLines(\"vis/index.html\"), collapse=\"\\n\"))\n\ncat('<iframe src=\"', tmp ,\n    '\" style=\"border: black; seamless:seamless; width: 800px; height: 200px\"></iframe>')\n```\n\n# References\n\nhttp://winvector.github.io/Parallel/PExample.html\nhttp://www.win-vector.com/blog/2016/01/parallel-computing-in-r/\nhttps://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/\nhttps://www.cs.princeton.edu/~blei/papers/Blei2012.pdf\nhttps://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/viewFile/4645/5021\nhttp://epub.wu.ac.at/3558/1/main.pdf\n\n# Session Info\n```{r}\nsessionInfo()\n```\n",
    "created" : 1459832254876.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "463|30|490|0|\n",
    "hash" : "2061411431",
    "id" : "EF72DC9D",
    "lastKnownWriteTime" : 1459437808,
    "last_content_update" : 1459844156637,
    "path" : "~/workspace/R/projects/ImmunoTopicModel/LDATopicModel.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}