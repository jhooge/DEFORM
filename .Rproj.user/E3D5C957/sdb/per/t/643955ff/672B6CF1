{
    "collab_server" : "",
    "contents" : "library(tm) ## texmining\nlibrary(lda) ## the actual LDA model\nlibrary(LDAvis) # visualization library for LDA\n\nlibrary(parallel) # multi-core paralellization\n\nlibrary(data.table) # fread\nlibrary(Rmpfr) # harmonic mean maximization\nlibrary(ggplot2) # pretty plotting lib\nlibrary(reshape2) # reformatting lib for ggplot2\n\nlibrary(tsne) # low dimensional embedding\nlibrary(caret) # ml model wrapper lib, but only used for data transformation here\n\nlibrary(rbokeh) # pretty (interactive) plotting\n\nload(\"ldaModels_NIPS2015_k2to25_alpha001to12.rda\")\nload(\"vocab.rda\")\nload(\"termTable.rda\")\nload(\"documents.rda\")\n\n\nbindToEnv <- function(bindTargetEnv=parent.frame(), objNames, doNotRebind=c()) {\n  # Bind the values into environment\n  # and switch any functions to this environment!\n  for(var in objNames) {\n    val <- get(var, envir=parent.frame())\n    if(is.function(val) && (!(var %in% doNotRebind))) {\n      # replace function's lexical environment with our target (DANGEROUS)\n      environment(val) <- bindTargetEnv\n    }\n    # assign object to target environment, only after any possible alteration\n    assign(var, val, envir=bindTargetEnv)\n  }\n}\n\nstartCluster <- function(cores=detectCores()) {\n  cluster <- makeCluster(cores)\n  return(cluster)\n}\n\nshutDownCluster <- function(cluster) {\n  if(!is.null(cluster)) {\n    stopCluster(cluster)\n    cluster <- c()\n  }\n}\n\n\n\npapers = fread(\"~/Datasets/kaggle/NIPS2015_papers/Papers.csv\")\ndocs = papers$Abstract\n\nD <- length(documents)  # number of documents\nW <- length(vocab)  # number of terms in the vocab\ndoc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document\nN <- sum(doc.length)  # total number of tokens in the data\nterm.frequency <- as.integer(term.table)  # frequencies of terms in the corpus\n\nG <- 100 ## number of iterations\n# alpha   ## Papers are very specific such that a low alpha is more probable.\n# eta     ## The scalar value of the Dirichlet hyperparamater for topic multinomials.\nk = seq(2, 25, by=1) ## The number of topics a document contains.\n\n## We will define parameter sets for 2400 LDA models, \n## which can be trained in about 1 hour using 8 cpu threadsâ€š\nparams <- expand.grid(G=G,\n                      k=k,\n                      alpha=seq(0.01, 12, length.out=10),\n                      eta=seq(0.01, 12, length.out=10))\n\nlogLiks = lapply(models, function(L)  L$log.likelihoods[1,])\nlogLiks.df <- as.data.frame(logLiks)\nmeanLogLiks <- data.frame(Iteration=1:nrow(logLiks.df), \n                          logLikelihood=rowMeans(logLiks.df))\nlogLiks.df$Iteration <- rownames(logLiks.df)\ncolnames(logLiks.df) <- 1:ncol(logLiks.df)\n\nmolten_logLiks <- melt(logLiks.df)\ncolnames(molten_logLiks) <- c(\"Iteration\", \"NumberOfTopics\", \"logLikelihood\")\nmolten_logLiks$Iteration <- as.numeric(molten_logLiks$Iteration)\n\n## Plot convergence\nggplot(data=molten_logLiks) + \n  geom_line(aes(x=Iteration, y=logLikelihood, color=NumberOfTopics), alpha=0.2, show.legend = FALSE) +\n  geom_line(data = meanLogLiks, aes(x=Iteration, y=logLikelihood), color=\"black\", linetype=\"dashed\") +\n  theme_bw()\n\n\nharmonicMean = function(logLikelihoods, precision=2000L) {\n  llMed = median(logLikelihoods)\n  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,\n                                       prec = precision) + llMed))))\n}\n\nparamMatrix <- function(k, params, models) {\n  topic.params <- params[which(params$k==k),]\n  indexes <- as.numeric(rownames(topic.params))\n  alpha <- unique(topic.params$alpha)\n  eta <- unique(topic.params$eta)\n  selectedModels <- models[indexes]\n  logLiks = lapply(selectedModels, function(m)  m$log.likelihoods[1, ])\n  harmMeanLogLiks <- sapply(logLiks, function(h) harmonicMean(h))\n  m <- matrix(harmMeanLogLiks, nrow=length(alpha), ncol=length(eta))\n  rownames(m) <- alpha\n  colnames(m) <- eta\n  return(m)\n}\n\n## contour matrices for each k over alpha and eta hyperparameters\ncontourMatrices <- lapply(unique(params$k), paramMatrix, params, models)\n## vector of argmax(likelihood) indices over all alphas of each contour matrix\nopt_alphas <- sapply(contourMatrices, function(m) which(rowSums(m)==max(rowSums(m))))\n## vector of argmax(likelihood) indices over all etas of each contour matrix\nopt_etas   <- sapply(contourMatrices, function(m) which(colSums(m)==max(colSums(m))))\nlogLiks    <- sapply(contourMatrices, function(m) max(apply(m, 1, max)))\n## Combine everything in a data frame including the optimal parameters over all LDAs with \n## a fixed k and number of iterations G\nopt_params <- data.frame(k=unique(params$k), \n                         alpha=names(opt_alphas), \n                         eta=names(opt_etas),\n                         logLikelihood=logLiks)\nopt_params\n\n## Plot harmonic mean log likelihood over all k\nggplot(data=opt_params, aes(x=k, y=logLikelihood)) +\n  geom_line() +\n  xlab(\"k\") +\n  ylab(\"Harmonic Mean log Likelihood\") +\n  theme_bw()\n\n## Estimate optimal parameter set\nparams <- as.data.frame(apply(params, 2, as.character))\n\ni <- which(opt_params$logLikelihood==max(opt_params$logLikelihood))\nopt_k <- as.character(opt_params[i, ]$k)\nopt_alpha <- as.character(opt_params[i, ]$alpha)\nopt_eta <- as.character(opt_params[i, ]$eta)\nopt_model_index <- as.numeric(rownames(subset(params, (k==opt_k & \n                                                       alpha==opt_alpha & \n                                                       eta==opt_eta))))\nopt_k <- as.numeric(opt_k)\nopt_alpha <- as.numeric(opt_alpha)\nopt_eta <- as.numeric(opt_eta)\nmodel <- models[[opt_model_index]]\n\n# alphaCurve <- function(k, eta, params) {\n#   opt_k <- as.character(opt_params[k, ]$k)\n#   opt_eta <- as.character(opt_params[k, ]$eta)\n#   k <- as.character(k)\n#   eta <- as.character(eta)\n#   \n#   alpha_model_indexes <- as.numeric(rownames(subset(params, (k==opt_k & eta==opt_eta))))\n#   alpha_model_logLiks <- lapply(models[alpha_model_indexes], function(m) m$log.likelihoods[1, ])\n#   alpha_model_harmMeanlogLiks <- sapply(alpha_model_logLiks, function(h) harmonicMean(h))\n#   return(alpha_model_harmMeanlogLiks)\n# }\n# \n# alphaCurve(2, opt_eta, params)\n\nm <- melt(contourMatrices[[i]])\ncolnames(m) <- c(\"alpha\", \"eta\", \"HarmonicMeanlogLikelihood\" )\n\n## Plot alpha, eta contour\nggplot(data=m, aes(x=alpha, y=eta, z=HarmonicMeanlogLikelihood)) +\n  geom_vline(xintercept = opt_alpha, colour=\"red\", linetype = \"longdash\", alpha=0.5) +\n  geom_hline(yintercept = opt_eta, colour=\"red\", linetype = \"longdash\", alpha=0.5) +\n  stat_contour(bins=20, aes(colour = ..level..)) +\n  theme_bw()\n\nN <- 5 \ntop.words <- top.topic.words(model$topics, 5, by.score=TRUE)\ntop.words.df <- as.data.frame(top.words)\ncolnames(top.words.df) <- 1:opt_k\n\ntop.words.df\n\n\ntop.documents <- top.topic.documents(model$document_sums, \n                                     num.documents = 20, \n                                     alpha = opt_alpha)\ntop.documents.df <- as.data.frame(top.documents)\ncolnames(top.documents.df) <- 1:opt_k\n\ntop.documents.df.part <- head(top.documents.df, 10)\ntopic_titles <- data.frame(lapply(1:opt_k, function(k) papers[as.numeric(top.documents.df.part[ ,k]),]$Title))\ncolnames(topic_titles) <- 1:opt_k\n\ntopic_titles\n\ntopic.proportions <- t(model$document_sums) / colSums(model$document_sums)\ntopic.proportions <- topic.proportions + 0.000000001 ## Laplace smoothing\n\ngetTopic <- function(topic.vec) {\n  argmax <- which(topic.vec==max(topic.vec))\n  if(length(argmax)>1){\n    argmax <- argmax[1]\n  }\n  return(argmax)\n}\n\ngetTopicProportion <- function(topic.vec) {\n  return(topic.vec[getTopic(topic.vec)])\n}\n\n# assign each document the topic with maximum probability\ndoc.topic <- unlist(apply(topic.proportions, 1, getTopic))\ndoc.maxTopicProportion <- unlist(apply(topic.proportions, 1, getTopicProportion))\ndoc.topic.words <- apply(top.words[, doc.topic], 2, paste, collapse=\".\")\n\n\nN <- 4\ntp <- topic.proportions[sample(1:dim(topic.proportions)[1], N),]\ncolnames(tp) <- apply(top.words, 2, paste, collapse=\" \")\ntp.df <- melt(cbind(data.frame(tp),\n                    document=factor(1:N)),\n              variable.name=\"topic\",\n              id.vars = \"document\")  \n\n\nggplot(data=tp.df, aes(x=topic, y=value, fill=document)) +\n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle=90, hjust=1)) +\n  coord_flip() +\n  facet_wrap(~ document, ncol=2) +\n  theme_bw()\n\n\nJSD <- function(p, q) {\n  m <- 0.5 * (p + q)\n  divergence <- 0.5 * (sum(p * log(p / m)) + sum(q * log(q / m)))\n  return(divergence)\n}\n\nn <- dim(topic.proportions)[1]\nX <- matrix(rep(0, n*n), nrow=n, ncol=n)\nindexes <- t(combn(1:nrow(topic.proportions), m=2))\nfor (r in 1:nrow(indexes)) {\n  i <- indexes[r, ][1]\n  j <- indexes[r, ][2]\n  p <- topic.proportions[i, ]\n  q <- topic.proportions[j, ]\n  X[i, j] <- JSD(p,q) \n}\nX <- X+t(X)\nX_dist <- sqrt(X) # compute Jensen-Shannon Distance\n\n## Projection by multidimensional scaling\nX_MDS_projected <- cmdscale(X_dist, k = 2) ## Multi dimensional scaling\n\nworker <- function() {\n  bindToEnv(objNames=c(\"topic.proportions\", \"opt_k\"))\n  function(perplexity) {\n    error <- capture.output(tsne::tsne(topic.proportions, k=2, \n                                       initial_dims=opt_k+1, \n                                       perplexity=perplexity),\n                            type=\"message\")\n    error <- unlist(strsplit(error, \" \"))\n    error <- as.numeric(error[length(error)])\n    X <- tsne::tsne(topic.proportions, k=2, \n                    initial_dims=opt_k+1, \n                    perplexity=perplexity)\n    result <- list(X, error)\n    return(result)\n  }\n}\n\nperplexities <- seq(5, 50, by=5)\nt1 <- Sys.time()\ncluster <- startCluster()\nX_tSNE_projections <- parLapply(cluster, perplexities, worker())\nshutDownCluster(cluster)\nt2 <- Sys.time()\nt2 - t1\n\n## Optimal t-SNE Projection\nerrors <- sapply(X_tSNE_projections, function(x) x[[2]])\nj <- which(errors==min(errors))\nminError <- errors[j]\nX_tSNE_projected <- X_tSNE_projections[[j]][[1]]\n\n## PCA Projection\npreProc = preProcess(topic.proportions, method=c(\"center\", \"scale\", \"pca\"))\nX_PCA_projected = predict(preProc, topic.proportions)[,1:2] # PCA projection\n\nprojections <- data.frame(Topic=as.factor(doc.topic), \n                          TopWords=as.factor(doc.topic.words),\n                          Proportion=doc.maxTopicProportion,\n                          Title=papers$Title,\n                          EventType=papers$EventType,\n                          Abstract=papers$Abstract,\n                          x_pca=X_PCA_projected[, 1], \n                          y_pca=X_PCA_projected[, 2],\n                          x_mds=X_MDS_projected[, 1], \n                          y_mds=X_MDS_projected[, 2],\n                          x_tsne=X_tSNE_projected[, 1], \n                          y_tsne=X_tSNE_projected[, 2])\n\n\ntools <- c(\"pan\", \n           \"wheel_zoom\", \"box_zoom\", \n           \"box_select\", \"lasso_select\", \n           \"reset\", \"save\")                            \n## PCA Plot\npca_fig <- figure(tools=tools, title=\"PCA\") %>%\n  ly_points(x_pca, y_pca, data = projections,\n            color = Topic, size = Proportion*10,\n            hover = list(TopWords, Proportion, Title, \n                         EventType))\n## MDS Plot\nmds_fig <- figure(tools=tools, title=\"MDS\") %>%\n  ly_points(x_mds, y_mds, data = projections,\n            color = Topic, size = Proportion*10,\n            hover = list(TopWords, Proportion, Title, \n                         EventType))\n## t-SNE Plot\ntsne_fig <- figure(tools=tools, title=sprintf(\"t-SNE (error=%.4f)\", minError)) %>%\n  ly_points(x_tsne, y_tsne, data = projections,\n            color = Topic, size = Proportion*10,\n            hover = list(TopWords, Proportion, Title, \n                         EventType))\n\nprojList <- list(pca_fig, mds_fig, tsne_fig)\np = grid_plot(projList, ncol=2, link_data=TRUE)\np\n",
    "created" : 1460366934066.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2915200957",
    "id" : "672B6CF1",
    "lastKnownWriteTime" : 1458723248,
    "last_content_update" : 1458723248,
    "path" : "~/workspace/R/projects/ImmunoTopicModel/generateFigures.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}